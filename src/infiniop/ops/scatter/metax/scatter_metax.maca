#include "scatter_metax.h"
#include "../../../devices/metax/metax_common.h"
#include "../../../devices/metax/metax_handle.h"
#include "../../../tensor.h"
#include "../../../../utils/custom_types.h"
#include <hcr/hc_runtime_api.h>

namespace op::scatter::metax {

Descriptor::~Descriptor() = default;

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle_,
    Descriptor **desc_ptr,
    infiniopTensorDescriptor_t input_desc,
    infiniopTensorDescriptor_t output_desc,
    infiniopTensorDescriptor_t index_desc,
    infiniopTensorDescriptor_t src_desc,
    int dim) {

    if (!handle_ || !desc_ptr || !input_desc || !output_desc || !index_desc || !src_desc) {
        return INFINI_STATUS_BAD_PARAM;
    }

    auto handle = static_cast<device::metax::Handle *>(handle_);
    
    // Get tensor shapes and strides
    auto input_shape = input_desc->shape();
    auto output_shape = output_desc->shape();
    auto index_shape = index_desc->shape();
    auto src_shape = src_desc->shape();
    auto input_strides = input_desc->strides();
    auto output_strides = output_desc->strides();
    auto index_strides = index_desc->strides();
    auto src_strides = src_desc->strides();
    
    // Validate dimensions
    if (dim < 0 || dim >= static_cast<int>(input_shape.size())) {
        return INFINI_STATUS_BAD_PARAM;
    }
    
    // Check data types
    auto input_dtype = input_desc->dtype();
    auto output_dtype = output_desc->dtype();
    auto src_dtype = src_desc->dtype();
    auto index_dtype = index_desc->dtype();
    
    if (input_dtype != output_dtype || input_dtype != src_dtype) {
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
    
    if (index_dtype != INFINI_DTYPE_I32 && index_dtype != INFINI_DTYPE_I64) {
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
    
    // Check supported data types
    if (input_dtype != INFINI_DTYPE_F16 && input_dtype != INFINI_DTYPE_F32 &&
        input_dtype != INFINI_DTYPE_F64 && input_dtype != INFINI_DTYPE_BF16 &&
        input_dtype != INFINI_DTYPE_I8 && input_dtype != INFINI_DTYPE_I16 &&
        input_dtype != INFINI_DTYPE_I32 && input_dtype != INFINI_DTYPE_I64 &&
        input_dtype != INFINI_DTYPE_U8 && input_dtype != INFINI_DTYPE_U16 &&
        input_dtype != INFINI_DTYPE_U32 && input_dtype != INFINI_DTYPE_U64 &&
        input_dtype != INFINI_DTYPE_BOOL) {
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
    
    // Validate shapes
    if (input_shape.size() != output_shape.size() || src_shape.size() != index_shape.size()) {
        return INFINI_STATUS_BAD_TENSOR_SHAPE;
    }
    
    *desc_ptr = new Descriptor(
        input_dtype, index_dtype,
        input_shape, output_shape, index_shape, src_shape,
        input_strides, output_strides, index_strides, src_strides,
        dim,
        handle->device, handle->device_id);
    
    return INFINI_STATUS_SUCCESS;
}

// Metax kernel for scatter operation
template<typename T, typename IndexT>
__global__ void scatterKernel(
    T *output,
    const T *src,
    const IndexT *index,
    size_t total_elements,
    size_t dim,
    size_t output_dim_size,
    const size_t *src_shape,
    const ptrdiff_t *src_strides,
    const ptrdiff_t *output_strides,
    const ptrdiff_t *index_strides,
    size_t ndim) {
    
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < total_elements) {
        // Convert linear index to multi-dimensional coordinates
        size_t temp_idx = idx;
        size_t coords[16]; // Support up to 16 dimensions
        
        // Bounds check for dimensions
        if (ndim > 16) return;
        
        for (int d = ndim - 1; d >= 0; d--) {
            coords[d] = temp_idx % src_shape[d];
            temp_idx /= src_shape[d];
        }
        
        // Calculate index offset first
        ptrdiff_t index_offset = 0;
        for (size_t d = 0; d < ndim; d++) {
            index_offset += coords[d] * index_strides[d];
        }
        
        // Get scatter index and perform bounds check
        IndexT scatter_index = index[index_offset];
        if (scatter_index >= 0 && scatter_index < static_cast<IndexT>(output_dim_size)) {
            // Calculate src offset
            size_t src_offset = 0;
            for (size_t d = 0; d < ndim; d++) {
                src_offset += coords[d] * src_strides[d];
            }
            
            // Calculate output offset
            size_t output_offset = 0;
            for (size_t d = 0; d < ndim; d++) {
                if (d == dim) {
                    output_offset += scatter_index * output_strides[d];
                } else {
                    output_offset += coords[d] * output_strides[d];
                }
            }
            
            // Copy data using direct element access
            output[output_offset] = src[src_offset];
        }
    }
}

infiniStatus_t Descriptor::calculate(
    void *workspace,
    size_t workspace_size,
    void *output,
    const void *input,
    const void *index,
    const void *src,
    void *stream) const {

    if (!output || !input || !index || !src) {
        return INFINI_STATUS_BAD_PARAM;
    }

    // Copy input to output first
    size_t output_elements = 1;
    for (size_t dim : _output_shape) {
        output_elements *= dim;
    }
    size_t output_size = output_elements * infiniSizeOf(_input_dtype);
    auto metax_stream = static_cast<hcStream_t>(stream);
    hcMemcpyAsync(output, input, output_size, hcMemcpyDeviceToDevice, metax_stream);
    
    // Dispatch based on data type and index type
    if (_index_dtype == INFINI_DTYPE_I32) {
        switch (_input_dtype) {
            case INFINI_DTYPE_F16:
                return scatterMetax<__half, int32_t>(output, src, index, stream);
            case INFINI_DTYPE_F32:
                return scatterMetax<float, int32_t>(output, src, index, stream);
            case INFINI_DTYPE_F64:
                return scatterMetax<double, int32_t>(output, src, index, stream);
            case INFINI_DTYPE_BF16:
                return scatterMetax<__hpcc_bfloat16, int32_t>(output, src, index, stream);
            case INFINI_DTYPE_I8:
                return scatterMetax<int8_t, int32_t>(output, src, index, stream);
            case INFINI_DTYPE_I16:
                return scatterMetax<int16_t, int32_t>(output, src, index, stream);
            case INFINI_DTYPE_I32:
                return scatterMetax<int32_t, int32_t>(output, src, index, stream);
            case INFINI_DTYPE_I64:
                return scatterMetax<int64_t, int32_t>(output, src, index, stream);
            case INFINI_DTYPE_U8:
                return scatterMetax<uint8_t, int32_t>(output, src, index, stream);
            case INFINI_DTYPE_U16:
                return scatterMetax<uint16_t, int32_t>(output, src, index, stream);
            case INFINI_DTYPE_U32:
                return scatterMetax<uint32_t, int32_t>(output, src, index, stream);
            case INFINI_DTYPE_U64:
                return scatterMetax<uint64_t, int32_t>(output, src, index, stream);
            case INFINI_DTYPE_BOOL:
                return scatterMetax<bool, int32_t>(output, src, index, stream);
            default:
                return INFINI_STATUS_BAD_TENSOR_DTYPE;
        }
    } else if (_index_dtype == INFINI_DTYPE_I64) {
        switch (_input_dtype) {
            case INFINI_DTYPE_F16:
                return scatterMetax<__half, int64_t>(output, src, index, stream);
            case INFINI_DTYPE_F32:
                return scatterMetax<float, int64_t>(output, src, index, stream);
            case INFINI_DTYPE_F64:
                return scatterMetax<double, int64_t>(output, src, index, stream);
            case INFINI_DTYPE_BF16:
                return scatterMetax<__hpcc_bfloat16, int64_t>(output, src, index, stream);
            case INFINI_DTYPE_I8:
                return scatterMetax<int8_t, int64_t>(output, src, index, stream);
            case INFINI_DTYPE_I16:
                return scatterMetax<int16_t, int64_t>(output, src, index, stream);
            case INFINI_DTYPE_I32:
                return scatterMetax<int32_t, int64_t>(output, src, index, stream);
            case INFINI_DTYPE_I64:
                return scatterMetax<int64_t, int64_t>(output, src, index, stream);
            case INFINI_DTYPE_U8:
                return scatterMetax<uint8_t, int64_t>(output, src, index, stream);
            case INFINI_DTYPE_U16:
                return scatterMetax<uint16_t, int64_t>(output, src, index, stream);
            case INFINI_DTYPE_U32:
                return scatterMetax<uint32_t, int64_t>(output, src, index, stream);
            case INFINI_DTYPE_U64:
                return scatterMetax<uint64_t, int64_t>(output, src, index, stream);
            case INFINI_DTYPE_BOOL:
                return scatterMetax<bool, int64_t>(output, src, index, stream);
            default:
                return INFINI_STATUS_BAD_TENSOR_DTYPE;
        }
    }
    
    return INFINI_STATUS_BAD_TENSOR_DTYPE;
}

template <typename T, typename IndexT>
infiniStatus_t Descriptor::scatterMetax(
    void *output_data,
    const void *src_data,
    const void *index_data,
    void *stream) const {

    auto metax_stream = static_cast<hcStream_t>(stream);
    
    // Calculate total elements in src
    size_t total_elements = 1;
    for (size_t dim : _src_shape) {
        total_elements *= dim;
    }
    
    if (total_elements == 0) {
        return INFINI_STATUS_SUCCESS;
    }
    
    // Allocate device memory for shape and stride arrays
    size_t *d_src_shape;
    ptrdiff_t *d_src_strides, *d_output_strides, *d_index_strides;
    
    hcMalloc((void**)&d_src_shape, _src_shape.size() * sizeof(size_t));
    hcMalloc((void**)&d_src_strides, _src_strides.size() * sizeof(ptrdiff_t));
    hcMalloc((void**)&d_output_strides, _output_strides.size() * sizeof(ptrdiff_t));
    hcMalloc((void**)&d_index_strides, _index_strides.size() * sizeof(ptrdiff_t));
    
    // Copy data to device
    hcMemcpyAsync(d_src_shape, _src_shape.data(), _src_shape.size() * sizeof(size_t), hcMemcpyHostToDevice, metax_stream);
    hcMemcpyAsync(d_src_strides, _src_strides.data(), _src_strides.size() * sizeof(ptrdiff_t), hcMemcpyHostToDevice, metax_stream);
    hcMemcpyAsync(d_output_strides, _output_strides.data(), _output_strides.size() * sizeof(ptrdiff_t), hcMemcpyHostToDevice, metax_stream);
    hcMemcpyAsync(d_index_strides, _index_strides.data(), _index_strides.size() * sizeof(ptrdiff_t), hcMemcpyHostToDevice, metax_stream);
    
    // Launch kernel
    const int block_size = 256;
    const int grid_size = (total_elements + block_size - 1) / block_size;
    
    scatterKernel<T, IndexT><<<grid_size, block_size, 0, metax_stream>>>(
        static_cast<T*>(output_data),
        static_cast<const T*>(src_data),
        static_cast<const IndexT*>(index_data),
        total_elements,
        _dim,
        _output_shape[_dim],
        d_src_shape,
        d_src_strides,
        d_output_strides,
        d_index_strides,
        _src_shape.size());
    
    // Wait for kernel completion
    hcStreamSynchronize(metax_stream);
    
    // Clean up device memory
    hcFree(d_src_shape);
    hcFree(d_src_strides);
    hcFree(d_output_strides);
    hcFree(d_index_strides);
    
    return INFINI_STATUS_SUCCESS;
}

} // namespace op::scatter::metax