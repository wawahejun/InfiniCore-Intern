#include "linear_backward_metax.h"
#include "../../../devices/metax/metax_common.h"
#include "../../../devices/metax/metax_handle.h"
#include "../../../tensor.h"
#include "../../../../utils/custom_types.h"
#include <hcr/hc_runtime_api.h>

namespace op::linear_backward::metax {

// Helper function to compute linear offset from multi-dimensional coordinates
__device__ __forceinline__ int compute_offset(const int* coords, const int* strides, int ndim) {
    int offset = 0;
    for (int i = 0; i < ndim; i++) {
        offset += coords[i] * strides[i];
    }
    return offset;
}

// Device-side type conversion functions
template<typename T>
__device__ __forceinline__ float device_cast_to_float(const T& val) {
    return static_cast<float>(val);
}

template<>
__device__ __forceinline__ float device_cast_to_float<fp16_t>(const fp16_t& val) {
    uint16_t h = val._v;
    uint32_t sign = (h & 0x8000) << 16;
    int32_t exponent = (h >> 10) & 0x1F;
    uint32_t mantissa = h & 0x3FF;

    uint32_t f32;
    if (exponent == 31) {
        if (mantissa != 0) {
            f32 = sign | 0x7F800000 | (mantissa << 13);
        } else {
            f32 = sign | 0x7F800000;
        }
    } else if (exponent == 0) {
        if (mantissa == 0) {
            f32 = sign;
        } else {
            exponent = -14;
            while ((mantissa & 0x400) == 0) {
                mantissa <<= 1;
                exponent--;
            }
            mantissa &= 0x3FF;
            f32 = sign | ((exponent + 127) << 23) | (mantissa << 13);
        }
    } else {
        f32 = sign | ((exponent + 127 - 15) << 23) | (mantissa << 13);
    }

    float result;
    memcpy(&result, &f32, sizeof(result));
    return result;
}

template<>
__device__ __forceinline__ float device_cast_to_float<bf16_t>(const bf16_t& val) {
    uint32_t bits32 = static_cast<uint32_t>(val._v) << 16;
    float result;
    memcpy(&result, &bits32, sizeof(result));
    return result;
}

template<typename T>
__device__ __forceinline__ T device_cast_from_float(float val) {
    return static_cast<T>(val);
}

template<>
__device__ __forceinline__ fp16_t device_cast_from_float<fp16_t>(float val) {
    uint32_t bits;
    memcpy(&bits, &val, sizeof(float));
    
    uint32_t sign = bits & 0x80000000;
    uint32_t exp = (bits & 0x7F800000) >> 23;
    uint32_t mantissa = bits & 0x007FFFFF;
    
    uint16_t result_bits;
    
    if (exp == 0) {
        result_bits = static_cast<uint16_t>(sign >> 16);
    } else if (exp == 0xFF) {
        result_bits = static_cast<uint16_t>((sign >> 16) | 0x7C00 | (mantissa ? 0x0200 : 0));
    } else {
        int new_exp = static_cast<int>(exp) - 127 + 15;
        if (new_exp <= 0) {
            result_bits = static_cast<uint16_t>(sign >> 16);
        } else if (new_exp >= 0x1F) {
            result_bits = static_cast<uint16_t>((sign >> 16) | 0x7C00);
        } else {
            result_bits = static_cast<uint16_t>((sign >> 16) | (new_exp << 10) | (mantissa >> 13));
        }
    }
    
    fp16_t result;
    memcpy(&result, &result_bits, sizeof(uint16_t));
    return result;
}

template<>
__device__ __forceinline__ bf16_t device_cast_from_float<bf16_t>(float val) {
    uint32_t bits32;
    memcpy(&bits32, &val, sizeof(bits32));
    const uint32_t rounding_bias = 0x00007FFF + ((bits32 >> 16) & 1);
    uint16_t bf16_bits = static_cast<uint16_t>((bits32 + rounding_bias) >> 16);
    return bf16_t{bf16_bits};
}

// Helper function to calculate strides
static std::vector<int> calculate_strides(const std::vector<int>& dims) {
    std::vector<int> strides(dims.size());
    if (dims.empty()) return strides;
    
    strides[dims.size() - 1] = 1;
    for (int i = dims.size() - 2; i >= 0; --i) {
        strides[i] = strides[i + 1] * dims[i + 1];
    }
    return strides;
}

Descriptor::~Descriptor() = default;

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle_,
    Descriptor **desc_ptr,
    infiniopTensorDescriptor_t grad_y_desc,
    infiniopTensorDescriptor_t x_desc,
    infiniopTensorDescriptor_t w_desc,
    infiniopTensorDescriptor_t grad_x_desc,
    infiniopTensorDescriptor_t grad_w_desc,
    infiniopTensorDescriptor_t grad_b_desc) {

    if (!handle_ || !desc_ptr || !grad_y_desc || !x_desc || !w_desc) {
        return INFINI_STATUS_BAD_PARAM;
    }

    auto handle = static_cast<device::metax::Handle *>(handle_);
    
    auto grad_y_dtype = grad_y_desc->dtype();
    auto x_dtype = x_desc->dtype();
    auto w_dtype = w_desc->dtype();
    
    // Check data types - support F16, F32, BF16
    if (grad_y_dtype != INFINI_DTYPE_F16 && grad_y_dtype != INFINI_DTYPE_F32 && grad_y_dtype != INFINI_DTYPE_BF16) {
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
    
    // Check that all input tensors have same dtype
    if (grad_y_dtype != x_dtype || grad_y_dtype != w_dtype) {
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
    
    // Check gradient tensor data types if provided
    if (grad_x_desc && grad_x_desc->dtype() != grad_y_dtype) {
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
    if (grad_w_desc && grad_w_desc->dtype() != grad_y_dtype) {
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
    if (grad_b_desc && grad_b_desc->dtype() != grad_y_dtype) {
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
    
    // Check dimensions
    auto grad_y_shape = grad_y_desc->shape();
    auto x_shape = x_desc->shape();
    auto w_shape = w_desc->shape();
    
    int grad_y_ndim = grad_y_shape.size();
    int x_ndim = x_shape.size();
    int w_ndim = w_shape.size();

    if (w_ndim != 2 || x_ndim < 1 || grad_y_ndim < 1) {
        return INFINI_STATUS_BAD_TENSOR_SHAPE;
    }

    // Get dimensions
    std::vector<int> grad_y_dims(grad_y_shape.begin(), grad_y_shape.end());
    std::vector<int> x_dims(x_shape.begin(), x_shape.end());
    std::vector<int> w_dims(w_shape.begin(), w_shape.end());

    // Check dimension compatibility
    // x: (..., in_features), w: (out_features, in_features), grad_y: (..., out_features)
    int in_features = x_dims[x_ndim - 1];
    int out_features = w_dims[0];
    int w_in_features = w_dims[1];

    if (in_features != w_in_features) {
        return INFINI_STATUS_BAD_TENSOR_SHAPE;
    }

    if (grad_y_dims[grad_y_ndim - 1] != out_features) {
        return INFINI_STATUS_BAD_TENSOR_SHAPE;
    }

    // Get actual strides from input tensor descriptors
    auto grad_y_strides_ptrdiff = grad_y_desc->strides();
    auto x_strides_ptrdiff = x_desc->strides();
    auto w_strides_ptrdiff = w_desc->strides();
    
    auto grad_y_strides = std::vector<int>(grad_y_strides_ptrdiff.begin(), grad_y_strides_ptrdiff.end());
    auto x_strides = std::vector<int>(x_strides_ptrdiff.begin(), x_strides_ptrdiff.end());
    auto w_strides = std::vector<int>(w_strides_ptrdiff.begin(), w_strides_ptrdiff.end());
    
    // Get output tensor dimensions and calculate their strides
    std::vector<int> grad_x_dims, grad_w_dims, grad_b_dims;
    std::vector<int> grad_x_strides, grad_w_strides, grad_b_strides;

    // Check gradient tensor dimensions if provided
    if (grad_x_desc) {
        auto grad_x_shape = grad_x_desc->shape();
        if (grad_x_shape != x_shape) {
            return INFINI_STATUS_BAD_TENSOR_SHAPE;
        }
        grad_x_dims = std::vector<int>(grad_x_shape.begin(), grad_x_shape.end());
        auto grad_x_strides_ptrdiff = grad_x_desc->strides();
        grad_x_strides = std::vector<int>(grad_x_strides_ptrdiff.begin(), grad_x_strides_ptrdiff.end());
    }
    
    if (grad_w_desc) {
        auto grad_w_shape = grad_w_desc->shape();
        if (grad_w_shape != w_shape) {
            return INFINI_STATUS_BAD_TENSOR_SHAPE;
        }
        grad_w_dims = std::vector<int>(grad_w_shape.begin(), grad_w_shape.end());
        auto grad_w_strides_ptrdiff = grad_w_desc->strides();
        grad_w_strides = std::vector<int>(grad_w_strides_ptrdiff.begin(), grad_w_strides_ptrdiff.end());
    }
    
    if (grad_b_desc) {
        auto grad_b_shape = grad_b_desc->shape();
        int grad_b_ndim = grad_b_shape.size();
        std::vector<int> grad_b_dims_temp(grad_b_shape.begin(), grad_b_shape.end());

        if (grad_b_ndim != 1 || grad_b_dims_temp[0] != out_features) {
            return INFINI_STATUS_BAD_TENSOR_SHAPE;
        }
        grad_b_dims = grad_b_dims_temp;
        auto grad_b_strides_ptrdiff = grad_b_desc->strides();
        grad_b_strides = std::vector<int>(grad_b_strides_ptrdiff.begin(), grad_b_strides_ptrdiff.end());
    }
    
    // Calculate batch size
    int batch_size = 1;
    for (size_t i = 0; i < x_dims.size() - 1; i++) {
        batch_size *= x_dims[i];
    }
    
    bool compute_grad_x = (grad_x_desc != nullptr);
    bool compute_grad_w = (grad_w_desc != nullptr);
    bool compute_grad_b = (grad_b_desc != nullptr);
    
    *desc_ptr = new Descriptor(
        grad_y_dtype, grad_y_dims, x_dims, w_dims,
        grad_x_dims, grad_w_dims, grad_b_dims,
        grad_y_strides, x_strides, w_strides,
        grad_x_strides, grad_w_strides, grad_b_strides,
        batch_size, in_features, out_features,
        compute_grad_x, compute_grad_w, compute_grad_b,
        handle->device, handle->device_id);
    
    return INFINI_STATUS_SUCCESS;
}

infiniStatus_t Descriptor::calculate(
    void *workspace,
    size_t workspace_size,
    void *grad_x,
    void *grad_w,
    void *grad_b,
    const void *grad_y,
    const void *x,
    const void *w,
    void *stream) const {

    if (!grad_y || !x || !w) {
        return INFINI_STATUS_BAD_PARAM;
    }
    
    if (_compute_grad_x && !grad_x) {
        return INFINI_STATUS_BAD_PARAM;
    }
    if (_compute_grad_w && !grad_w) {
        return INFINI_STATUS_BAD_PARAM;
    }
    if (_compute_grad_b && !grad_b) {
        return INFINI_STATUS_BAD_PARAM;
    }

    // Dispatch based on data type
    switch (_dtype) {
        case INFINI_DTYPE_F16:
            return linearBackwardMetax<fp16_t>(grad_x, grad_w, grad_b, grad_y, x, w, stream);
        case INFINI_DTYPE_F32:
            return linearBackwardMetax<float>(grad_x, grad_w, grad_b, grad_y, x, w, stream);
        case INFINI_DTYPE_BF16:
            return linearBackwardMetax<bf16_t>(grad_x, grad_w, grad_b, grad_y, x, w, stream);
        default:
            return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
}

// Metax kernel for computing grad_x
template<typename T>
__global__ void linearBackwardGradXKernel(
    T *grad_x_data,
    const T *grad_y_data,
    const T *w_data,
    const int *grad_x_shape,
    const int *grad_y_shape,
    const int *w_shape,
    const int *grad_x_strides,
    const int *grad_y_strides,
    const int *w_strides,
    int grad_x_ndim,
    int grad_y_ndim,
    int w_ndim,
    int total_elements) {
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < total_elements) {
        // Convert linear index to multi-dimensional coordinates for grad_x
        int grad_x_coords[8]; // Support up to 8 dimensions
        int temp_idx = idx;
        for (int i = grad_x_ndim - 1; i >= 0; i--) {
            grad_x_coords[i] = temp_idx % grad_x_shape[i];
            temp_idx /= grad_x_shape[i];
        }
        
        // Get input feature index (last dimension)
        int in_idx = grad_x_coords[grad_x_ndim - 1];
        
        // Compute grad_y coordinates (same as grad_x except last dimension)
        int grad_y_coords[8];
        for (int i = 0; i < grad_y_ndim - 1; i++) {
            grad_y_coords[i] = grad_x_coords[i];
        }
        
        float sum = 0.0f;
        
        // Compute grad_x = grad_y * w^T
        int out_features = grad_y_shape[grad_y_ndim - 1];
        for (int out_idx = 0; out_idx < out_features; out_idx++) {
            // Set output feature coordinate
            grad_y_coords[grad_y_ndim - 1] = out_idx;
            
            // Compute grad_y offset
            int grad_y_offset = compute_offset(grad_y_coords, grad_y_strides, grad_y_ndim);
            
            // Compute w offset: w[out_idx][in_idx]
            int w_coords[2] = {out_idx, in_idx};
            int w_offset = compute_offset(w_coords, w_strides, w_ndim);
            
            T grad_y_val = grad_y_data[grad_y_offset];
            T w_val = w_data[w_offset];
            sum += device_cast_to_float(grad_y_val) * device_cast_to_float(w_val);
        }
        
        // Compute grad_x offset and store result
        int grad_x_offset = compute_offset(grad_x_coords, grad_x_strides, grad_x_ndim);
        grad_x_data[grad_x_offset] = device_cast_from_float<T>(sum);
    }
}

// Metax kernel for computing grad_w
template<typename T>
__global__ void linearBackwardGradWKernel(
    T *grad_w_data,
    const T *grad_y_data,
    const T *x_data,
    const int *grad_w_shape,
    const int *grad_y_shape,
    const int *x_shape,
    const int *grad_w_strides,
    const int *grad_y_strides,
    const int *x_strides,
    int grad_w_ndim,
    int grad_y_ndim,
    int x_ndim,
    int total_elements) {
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < total_elements) {
        // Convert linear index to multi-dimensional coordinates for grad_w
        int grad_w_coords[8]; // Support up to 8 dimensions
        int temp_idx = idx;
        for (int i = grad_w_ndim - 1; i >= 0; i--) {
            grad_w_coords[i] = temp_idx % grad_w_shape[i];
            temp_idx /= grad_w_shape[i];
        }
        
        // Get weight indices: grad_w[out_idx][in_idx]
        int out_idx = grad_w_coords[0];
        int in_idx = grad_w_coords[1];
        
        float sum = 0.0f;
        
        // Compute grad_w = grad_y^T * x
        // Calculate batch size from x dimensions
        int batch_size = 1;
        for (int i = 0; i < x_ndim - 1; i++) {
            batch_size *= x_shape[i];
        }
        
        // Iterate over all batch elements
        for (int batch_linear_idx = 0; batch_linear_idx < batch_size; batch_linear_idx++) {
            // Convert batch linear index to coordinates
            int batch_coords[8];
            int temp_batch_idx = batch_linear_idx;
            for (int i = x_ndim - 2; i >= 0; i--) {
                batch_coords[i] = temp_batch_idx % x_shape[i];
                temp_batch_idx /= x_shape[i];
            }
            
            // Compute grad_y coordinates: [...batch_dims, out_idx]
            int grad_y_coords[8];
            for (int i = 0; i < grad_y_ndim - 1; i++) {
                grad_y_coords[i] = batch_coords[i];
            }
            grad_y_coords[grad_y_ndim - 1] = out_idx;
            
            // Compute x coordinates: [...batch_dims, in_idx]
            int x_coords[8];
            for (int i = 0; i < x_ndim - 1; i++) {
                x_coords[i] = batch_coords[i];
            }
            x_coords[x_ndim - 1] = in_idx;
            
            // Compute offsets
            int grad_y_offset = compute_offset(grad_y_coords, grad_y_strides, grad_y_ndim);
            int x_offset = compute_offset(x_coords, x_strides, x_ndim);
            
            T grad_y_val = grad_y_data[grad_y_offset];
            T x_val = x_data[x_offset];
            sum += device_cast_to_float(grad_y_val) * device_cast_to_float(x_val);
        }
        
        // Compute grad_w offset and store result
        int grad_w_offset = compute_offset(grad_w_coords, grad_w_strides, grad_w_ndim);
        grad_w_data[grad_w_offset] = device_cast_from_float<T>(sum);
    }
}

// Metax kernel for computing grad_b
template<typename T>
__global__ void linearBackwardGradBKernel(
    T *grad_b_data,
    const T *grad_y_data,
    const int *grad_b_shape,
    const int *grad_y_shape,
    const int *grad_b_strides,
    const int *grad_y_strides,
    int grad_b_ndim,
    int grad_y_ndim,
    int total_elements) {
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < total_elements) {
        // Convert linear index to multi-dimensional coordinates for grad_b
        int grad_b_coords[8]; // Support up to 8 dimensions
        int temp_idx = idx;
        for (int i = grad_b_ndim - 1; i >= 0; i--) {
            grad_b_coords[i] = temp_idx % grad_b_shape[i];
            temp_idx /= grad_b_shape[i];
        }
        
        // Get output feature index (should be the only dimension for bias)
        int out_idx = grad_b_coords[0];
        
        float sum = 0.0f;
        
        // Compute grad_b = sum(grad_y, dim=0) - sum over all batch dimensions
        // Calculate batch size from grad_y dimensions
        int batch_size = 1;
        for (int i = 0; i < grad_y_ndim - 1; i++) {
            batch_size *= grad_y_shape[i];
        }
        
        // Iterate over all batch elements
        for (int batch_linear_idx = 0; batch_linear_idx < batch_size; batch_linear_idx++) {
            // Convert batch linear index to coordinates
            int batch_coords[8];
            int temp_batch_idx = batch_linear_idx;
            for (int i = grad_y_ndim - 2; i >= 0; i--) {
                batch_coords[i] = temp_batch_idx % grad_y_shape[i];
                temp_batch_idx /= grad_y_shape[i];
            }
            
            // Compute grad_y coordinates: [...batch_dims, out_idx]
            int grad_y_coords[8];
            for (int i = 0; i < grad_y_ndim - 1; i++) {
                grad_y_coords[i] = batch_coords[i];
            }
            grad_y_coords[grad_y_ndim - 1] = out_idx;
            
            // Compute grad_y offset
            int grad_y_offset = compute_offset(grad_y_coords, grad_y_strides, grad_y_ndim);
            
            sum += device_cast_to_float(grad_y_data[grad_y_offset]);
        }
        
        // Compute grad_b offset and store result
        int grad_b_offset = compute_offset(grad_b_coords, grad_b_strides, grad_b_ndim);
        grad_b_data[grad_b_offset] = device_cast_from_float<T>(sum);
    }
}

template <typename T>
infiniStatus_t Descriptor::linearBackwardMetax(
    void *grad_x_data,
    void *grad_w_data,
    void *grad_b_data,
    const void *grad_y_data,
    const void *x_data,
    const void *w_data,
    void *stream) const {

    auto metax_stream = static_cast<hcStream_t>(stream);
    
    // Allocate device memory for shapes and strides
    int *d_grad_y_shape, *d_x_shape, *d_w_shape;
    int *d_grad_y_strides, *d_x_strides, *d_w_strides;
    int *d_grad_x_shape = nullptr, *d_grad_w_shape = nullptr, *d_grad_b_shape = nullptr;
    int *d_grad_x_strides = nullptr, *d_grad_w_strides = nullptr, *d_grad_b_strides = nullptr;
    
    size_t grad_y_shape_size = _grad_y_dims.size() * sizeof(int);
    size_t x_shape_size = _x_dims.size() * sizeof(int);
    size_t w_shape_size = _w_dims.size() * sizeof(int);
    
    // Allocate common shapes and strides
    hcMalloc((void**)&d_grad_y_shape, grad_y_shape_size);
    hcMalloc((void**)&d_x_shape, x_shape_size);
    hcMalloc((void**)&d_w_shape, w_shape_size);
    hcMalloc((void**)&d_grad_y_strides, grad_y_shape_size);
    hcMalloc((void**)&d_x_strides, x_shape_size);
    hcMalloc((void**)&d_w_strides, w_shape_size);
    
    // Allocate grad_b shapes and strides if needed
    if (_compute_grad_b && !_grad_b_dims.empty()) {
        size_t grad_b_shape_size = _grad_b_dims.size() * sizeof(int);
        hcMalloc((void**)&d_grad_b_shape, grad_b_shape_size);
        hcMalloc((void**)&d_grad_b_strides, grad_b_shape_size);
        hcMemcpyAsync(d_grad_b_shape, _grad_b_dims.data(), grad_b_shape_size, hcMemcpyHostToDevice, metax_stream);
        hcMemcpyAsync(d_grad_b_strides, _grad_b_strides.data(), grad_b_shape_size, hcMemcpyHostToDevice, metax_stream);
    }
    
    // Copy common shapes and strides to device
    hcMemcpyAsync(d_grad_y_shape, _grad_y_dims.data(), grad_y_shape_size, hcMemcpyHostToDevice, metax_stream);
    hcMemcpyAsync(d_x_shape, _x_dims.data(), x_shape_size, hcMemcpyHostToDevice, metax_stream);
    hcMemcpyAsync(d_w_shape, _w_dims.data(), w_shape_size, hcMemcpyHostToDevice, metax_stream);
    hcMemcpyAsync(d_grad_y_strides, _grad_y_strides.data(), grad_y_shape_size, hcMemcpyHostToDevice, metax_stream);
    hcMemcpyAsync(d_x_strides, _x_strides.data(), x_shape_size, hcMemcpyHostToDevice, metax_stream);
    hcMemcpyAsync(d_w_strides, _w_strides.data(), w_shape_size, hcMemcpyHostToDevice, metax_stream);
    
    const int block_size = 256;
    
    // Compute grad_x if needed
    if (_compute_grad_x) {
        size_t grad_x_shape_size = _grad_x_dims.size() * sizeof(int);
        hcMalloc((void**)&d_grad_x_shape, grad_x_shape_size);
        hcMalloc((void**)&d_grad_x_strides, grad_x_shape_size);
        hcMemcpyAsync(d_grad_x_shape, _grad_x_dims.data(), grad_x_shape_size, hcMemcpyHostToDevice, metax_stream);
        hcMemcpyAsync(d_grad_x_strides, _grad_x_strides.data(), grad_x_shape_size, hcMemcpyHostToDevice, metax_stream);
        
        int total_elements = 1;
        for (int dim : _grad_x_dims) {
            total_elements *= dim;
        }
        int grid_size = (total_elements + block_size - 1) / block_size;
        
        linearBackwardGradXKernel<T><<<grid_size, block_size, 0, metax_stream>>>(
            static_cast<T*>(grad_x_data),
            static_cast<const T*>(grad_y_data),
            static_cast<const T*>(w_data),
            d_grad_x_shape,
            d_grad_y_shape,
            d_w_shape,
            d_grad_x_strides,
            d_grad_y_strides,
            d_w_strides,
            static_cast<int>(_grad_x_dims.size()),
            static_cast<int>(_grad_y_dims.size()),
            static_cast<int>(_w_dims.size()),
            total_elements);
    }
    
    // Compute grad_w if needed
    if (_compute_grad_w) {
        size_t grad_w_shape_size = _grad_w_dims.size() * sizeof(int);
        hcMalloc((void**)&d_grad_w_shape, grad_w_shape_size);
        hcMalloc((void**)&d_grad_w_strides, grad_w_shape_size);
        hcMemcpyAsync(d_grad_w_shape, _grad_w_dims.data(), grad_w_shape_size, hcMemcpyHostToDevice, metax_stream);
        hcMemcpyAsync(d_grad_w_strides, _grad_w_strides.data(), grad_w_shape_size, hcMemcpyHostToDevice, metax_stream);
        
        int total_elements = 1;
        for (int dim : _grad_w_dims) {
            total_elements *= dim;
        }
        int grid_size = (total_elements + block_size - 1) / block_size;
        
        linearBackwardGradWKernel<T><<<grid_size, block_size, 0, metax_stream>>>(
            static_cast<T*>(grad_w_data),
            static_cast<const T*>(grad_y_data),
            static_cast<const T*>(x_data),
            d_grad_w_shape,
            d_grad_y_shape,
            d_x_shape,
            d_grad_w_strides,
            d_grad_y_strides,
            d_x_strides,
            static_cast<int>(_grad_w_dims.size()),
            static_cast<int>(_grad_y_dims.size()),
            static_cast<int>(_x_dims.size()),
            total_elements);
    }
    
    // Compute grad_b if needed
    if (_compute_grad_b) {
        int total_elements = 1;
        for (int dim : _grad_b_dims) {
            total_elements *= dim;
        }
        int grid_size = (total_elements + block_size - 1) / block_size;
        
        linearBackwardGradBKernel<T><<<grid_size, block_size, 0, metax_stream>>>(
            static_cast<T*>(grad_b_data),
            static_cast<const T*>(grad_y_data),
            d_grad_b_shape,
            d_grad_y_shape,
            d_grad_b_strides,
            d_grad_y_strides,
            static_cast<int>(_grad_b_dims.size()),
            static_cast<int>(_grad_y_dims.size()),
            total_elements);
    }
    
    // Cleanup device memory
    hcFree(d_grad_y_shape);
    hcFree(d_x_shape);
    hcFree(d_w_shape);
    hcFree(d_grad_y_strides);
    hcFree(d_x_strides);
    hcFree(d_w_strides);
    
    if (_compute_grad_x) {
        hcFree(d_grad_x_shape);
        hcFree(d_grad_x_strides);
    }
    if (_compute_grad_w) {
        hcFree(d_grad_w_shape);
        hcFree(d_grad_w_strides);
    }
    if (_compute_grad_b && d_grad_b_shape) {
        hcFree(d_grad_b_shape);
        hcFree(d_grad_b_strides);
    }
    
    return INFINI_STATUS_SUCCESS;
}

} // namespace op::linear_backward::metax