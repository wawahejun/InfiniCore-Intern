#include "layer_norm_metax.h"
#include "../../../devices/metax/metax_handle.h"
#include "../../../devices/metax/metax_common.h"
#include "../../../devices/metax/metax_kernel_common.h"
#include "../../../tensor.h"
#include "../../../../utils/custom_types.h"

// 设备端数据类型转换函数
template<typename T>
__device__ float device_cast_to_float(T val) {
    return static_cast<float>(val);
}

template<>
__device__ float device_cast_to_float<fp16_t>(fp16_t val) {
    // Convert custom fp16_t to __half first, then to float
    __half h_val;
    memcpy(&h_val, &val, sizeof(__half));
    return __half2float(h_val);
}

template<>
__device__ float device_cast_to_float<bf16_t>(bf16_t val) {
    // Convert custom bf16_t to __hpcc_bfloat16 first, then to float
    __hpcc_bfloat16 bf_val;
    memcpy(&bf_val, &val, sizeof(__hpcc_bfloat16));
    return __bfloat162float(bf_val);
}

template<typename T>
__device__ T device_cast_from_float(float val) {
    return static_cast<T>(val);
}

template<>
__device__ fp16_t device_cast_from_float<fp16_t>(float val) {
    // Convert float to __half first, then to custom fp16_t
    __half h_val = __float2half(val);
    fp16_t result;
    memcpy(&result, &h_val, sizeof(fp16_t));
    return result;
}

template<>
__device__ bf16_t device_cast_from_float<bf16_t>(float val) {
    // Convert float to __hpcc_bfloat16 first, then to custom bf16_t
    __hpcc_bfloat16 bf_val = __float2bfloat16(val);
    bf16_t result;
    memcpy(&result, &bf_val, sizeof(bf16_t));
    return result;
}

// LayerNorm CUDA核函数
template<unsigned int BLOCK_SIZE, typename T>
__global__ void layerNormKernel(
    T *__restrict__ output,
    const T *__restrict__ input,
    const T *__restrict__ weight,
    const T *__restrict__ bias,
    T *__restrict__ input_std_deviation,
    T *__restrict__ input_standardization,
    size_t batch_size,
    size_t normalized_size,
    float eps,
    bool has_bias) {
    
    int batch_idx = blockIdx.x;
    if (batch_idx >= batch_size) return;
    
    int tid = threadIdx.x;
    
    // 计算当前batch的输入起始位置
    const T* batch_input = input + batch_idx * normalized_size;
    T* batch_output = output + batch_idx * normalized_size;
    T* batch_standardization = input_standardization + batch_idx * normalized_size;
    
    // 使用共享内存进行reduction
    __shared__ float shared_sum[BLOCK_SIZE];
    __shared__ float shared_sum_sq[BLOCK_SIZE];
    
    // 计算均值
    float sum = 0.0f;
    for (int i = tid; i < normalized_size; i += BLOCK_SIZE) {
        sum += device_cast_to_float(batch_input[i]);
    }
    shared_sum[tid] = sum;
    __syncthreads();
    
    // Reduction求和
    for (int stride = BLOCK_SIZE / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            shared_sum[tid] += shared_sum[tid + stride];
        }
        __syncthreads();
    }
    
    float mean = shared_sum[0] / normalized_size;
    __syncthreads();
    
    // 计算方差
    float sum_sq = 0.0f;
    for (int i = tid; i < normalized_size; i += BLOCK_SIZE) {
        float diff = device_cast_to_float(batch_input[i]) - mean;
        sum_sq += diff * diff;
    }
    shared_sum_sq[tid] = sum_sq;
    __syncthreads();
    
    // Reduction求和
    for (int stride = BLOCK_SIZE / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            shared_sum_sq[tid] += shared_sum_sq[tid + stride];
        }
        __syncthreads();
    }
    
    float variance = shared_sum_sq[0] / normalized_size;
    float std_dev = sqrtf(variance + eps);
    
    // 存储标准差
    if (tid == 0) {
        input_std_deviation[batch_idx] = device_cast_from_float<T>(std_dev);
    }
    
    // 归一化并应用权重和偏置
    for (int i = tid; i < normalized_size; i += BLOCK_SIZE) {
        float normalized = (device_cast_to_float(batch_input[i]) - mean) / std_dev;
        
        // 存储标准化结果
        batch_standardization[i] = device_cast_from_float<T>(normalized);
        
        // 应用权重和偏置
        float result = normalized * device_cast_to_float(weight[i]);
        if (has_bias) {
            result += device_cast_to_float(bias[i]);
        }
        
        batch_output[i] = device_cast_from_float<T>(result);
    }
}

// 启动LayerNorm核函数
template<unsigned int BLOCK_SIZE>
void launchLayerNormKernel(
    const op::layer_norm::LayerNormInfo &info,
    void *output,
    const void *input,
    const void *weight,
    const void *bias,
    void *input_std_deviation,
    void *input_standardization,
    void *stream) {
    
    dim3 grid(info.batch_size());
    dim3 block(BLOCK_SIZE);
    
    auto cuda_stream = reinterpret_cast<hcStream_t>(stream);
    
    switch (info.dtype) {
    case INFINI_DTYPE_F32:
        layerNormKernel<BLOCK_SIZE, float><<<grid, block, 0, cuda_stream>>>(
            reinterpret_cast<float*>(output),
            reinterpret_cast<const float*>(input),
            reinterpret_cast<const float*>(weight),
            reinterpret_cast<const float*>(bias),
            reinterpret_cast<float*>(input_std_deviation),
            reinterpret_cast<float*>(input_standardization),
            info.batch_size(), info.dim(),
            info.eps, info.has_bias);
        break;
    case INFINI_DTYPE_F16:
        layerNormKernel<BLOCK_SIZE, fp16_t><<<grid, block, 0, cuda_stream>>>(
            reinterpret_cast<fp16_t*>(output),
            reinterpret_cast<const fp16_t*>(input),
            reinterpret_cast<const fp16_t*>(weight),
            reinterpret_cast<const fp16_t*>(bias),
            reinterpret_cast<fp16_t*>(input_std_deviation),
            reinterpret_cast<fp16_t*>(input_standardization),
            info.batch_size(), info.dim(),
            info.eps, info.has_bias);
        break;
    case INFINI_DTYPE_BF16:
        layerNormKernel<BLOCK_SIZE, bf16_t><<<grid, block, 0, cuda_stream>>>(
            reinterpret_cast<bf16_t*>(output),
            reinterpret_cast<const bf16_t*>(input),
            reinterpret_cast<const bf16_t*>(weight),
            reinterpret_cast<const bf16_t*>(bias),
            reinterpret_cast<bf16_t*>(input_std_deviation),
            reinterpret_cast<bf16_t*>(input_standardization),
            info.batch_size(), info.dim(),
            info.eps, info.has_bias);
        break;
    default:
        // 不支持的数据类型
        break;
    }
}

namespace op::layer_norm::metax {

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle_,
    Descriptor **desc_ptr,
    infiniopTensorDescriptor_t output_desc,
    infiniopTensorDescriptor_t input_desc,
    infiniopTensorDescriptor_t weight_desc,
    infiniopTensorDescriptor_t bias_desc,
    infiniopTensorDescriptor_t input_std_deviation_desc,
    infiniopTensorDescriptor_t input_standardization_desc,
    float eps) {
    
    // 验证输入参数
    if (!handle_ || !desc_ptr || !output_desc || !input_desc || 
        !weight_desc || !input_std_deviation_desc || !input_standardization_desc) {
        return INFINI_STATUS_BAD_PARAM;
    }
    
    if (eps <= 0.0f) {
        return INFINI_STATUS_BAD_PARAM;
    }
    
    auto handle = reinterpret_cast<device::metax::Handle *>(handle_);
    auto dtype = input_desc->dtype();
    
    // 检查数据类型支持
    if (dtype != INFINI_DTYPE_F16 && dtype != INFINI_DTYPE_F32 && dtype != INFINI_DTYPE_BF16) {
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
    
    // 检查数据类型一致性
    if (input_desc->dtype() != output_desc->dtype() ||
        input_desc->dtype() != input_std_deviation_desc->dtype() ||
        input_desc->dtype() != input_standardization_desc->dtype()) {
        return INFINI_STATUS_BAD_PARAM;
    }
    
    // 检查输入维度（至少1D）
        if (input_desc->ndim() < 1) {
            return INFINI_STATUS_BAD_TENSOR_SHAPE;
        }
    
    // 检查形状兼容性
    if (input_desc->ndim() != output_desc->ndim() ||
        input_desc->ndim() != input_standardization_desc->ndim()) {
        return INFINI_STATUS_BAD_PARAM;
    }
    
    for (size_t i = 0; i < input_desc->ndim(); ++i) {
        if (input_desc->dim(i) != output_desc->dim(i) ||
            input_desc->dim(i) != input_standardization_desc->dim(i)) {
            return INFINI_STATUS_BAD_PARAM;
        }
    }
    
    // weight应该是1D张量，长度为最后一维
    size_t normalized_size = input_desc->dim(input_desc->ndim() - 1);
    if (weight_desc->ndim() != 1 || weight_desc->dim(0) != normalized_size) {
        return INFINI_STATUS_BAD_PARAM;
    }
    
    // 检查bias形状（如果存在）
    bool has_bias = (bias_desc != nullptr);
    if (has_bias) {
        if (bias_desc->ndim() != 1 || bias_desc->dim(0) != normalized_size) {
            return INFINI_STATUS_BAD_PARAM;
        }
    }
    
    // 检查input_std_deviation形状（应该是input去掉最后一维）
    size_t expected_std_ndim = (input_desc->ndim() == 1) ? 0 : input_desc->ndim() - 1;
    if (input_std_deviation_desc->ndim() != expected_std_ndim) {
        return INFINI_STATUS_BAD_PARAM;
    }
    for (size_t i = 0; i < input_std_deviation_desc->ndim(); ++i) {
        if (input_std_deviation_desc->dim(i) != input_desc->dim(i)) {
            return INFINI_STATUS_BAD_PARAM;
        }
    }
    
    // 创建LayerNormInfo
    LayerNormInfo info;
    
    // 计算batch_size（除了最后一维的所有维度的乘积）
    info._batch_size = 1;
    for (size_t i = 0; i < input_desc->ndim() - 1; ++i) {
        info._batch_size *= input_desc->dim(i);
    }
    
    info._normalized_size = normalized_size;
    info.total_elements = input_desc->numel();
    info.input_size = input_desc->numel();
    info.output_size = output_desc->numel();
    info.dtype = dtype;
    info.eps = eps;
    info.has_bias = has_bias;
    
    // 复制形状和步长信息
    info.input_shape = input_desc->shape();
    info.output_shape = output_desc->shape();
    info.weight_shape = weight_desc->shape();
    if (has_bias) {
        info.bias_shape = bias_desc->shape();
    }
    info.input_std_deviation_shape = input_std_deviation_desc->shape();
    info.input_standardization_shape = input_standardization_desc->shape();
    
    info.input_strides = input_desc->strides();
    info.output_strides = output_desc->strides();
    info.weight_strides = weight_desc->strides();
    if (has_bias) {
        info.bias_strides = bias_desc->strides();
    }
    info.input_std_deviation_strides = input_std_deviation_desc->strides();
    info.input_standardization_strides = input_standardization_desc->strides();
    
    // LayerNorm不需要额外的workspace
    size_t workspace_size = 0;
    
    *desc_ptr = new Descriptor(
        std::move(info),
        workspace_size,
        handle->device,
        handle->device_id);
    
    return INFINI_STATUS_SUCCESS;
}

infiniStatus_t Descriptor::get_workspace_size(size_t *size) const {
    if (!size) {
        return INFINI_STATUS_BAD_PARAM;
    }
    *size = _workspace_size;
    return INFINI_STATUS_SUCCESS;
}

infiniStatus_t Descriptor::calculate(
    void *workspace, size_t workspace_size,
    void *output,
    const void *input,
    const void *weight,
    const void *bias,
    void *input_std_deviation,
    void *input_standardization,
    void *stream) const {
    
    if (!output || !input || !weight || !input_std_deviation || !input_standardization) {
        return INFINI_STATUS_BAD_PARAM;
    }
    
    if (info.has_bias && !bias) {
        return INFINI_STATUS_BAD_PARAM;
    }
    
    // 根据数据类型调用相应的模板函数
    switch (info.dtype) {
    case INFINI_DTYPE_F32:
        return layerNormMetax<float>(info, output, input, weight, bias,
                                   input_std_deviation, input_standardization, stream);
    case INFINI_DTYPE_F16:
        return layerNormMetax<fp16_t>(info, output, input, weight, bias,
                                    input_std_deviation, input_standardization, stream);
    case INFINI_DTYPE_BF16:
        return layerNormMetax<bf16_t>(info, output, input, weight, bias,
                                     input_std_deviation, input_standardization, stream);
    default:
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }
}

// 模板函数实现
template<typename T>
infiniStatus_t layerNormMetax(
    const LayerNormInfo &info,
    void *output,
    const void *input,
    const void *weight,
    const void *bias,
    void *input_std_deviation,
    void *input_standardization,
    void *stream) {
    
    // 使用固定的block size
    constexpr unsigned int BLOCK_SIZE = 256;
    launchLayerNormKernel<BLOCK_SIZE>(
        info, output, input, weight, bias,
        input_std_deviation, input_standardization, stream);
    
    // 检查错误
     CHECK_METAX(hcGetLastError());
    
    return INFINI_STATUS_SUCCESS;
}

// 显式实例化模板
template infiniStatus_t layerNormMetax<float>(
    const LayerNormInfo &info,
    void *output,
    const void *input,
    const void *weight,
    const void *bias,
    void *input_std_deviation,
    void *input_standardization,
    void *stream);

template infiniStatus_t layerNormMetax<fp16_t>(
    const LayerNormInfo &info,
    void *output,
    const void *input,
    const void *weight,
    const void *bias,
    void *input_std_deviation,
    void *input_standardization,
    void *stream);

template infiniStatus_t layerNormMetax<bf16_t>(
    const LayerNormInfo &info,
    void *output,
    const void *input,
    const void *weight,
    const void *bias,
    void *input_std_deviation,
    void *input_standardization,
    void *stream);

} // namespace op::layer_norm::metax