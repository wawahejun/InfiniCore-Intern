#include "sigmoid_backward_metax.h"

#include "../../../elementwise/metax/elementwise_metax.h"
#include "../../../../utils/custom_types.h"

namespace op::sigmoid_backward::metax {

// High precision sigmoid function implementation
template<typename T>
__device__ __forceinline__ T sigmoid_func(T x) {
    if constexpr (std::is_same_v<T, half>) {
        // For half type, use built-in functions
        return __hdiv(__float2half(1.0f), __hadd(__float2half(1.0f), hexp(__hneg(x))));
    } else if constexpr (std::is_same_v<T, cuda_bfloat16>) {
        // For bfloat16, convert to float for higher precision
        float x_float = __bfloat162float(x);
        float result = 1.0f / (1.0f + expf(-x_float));
        return __float2bfloat16(result);
    } else if constexpr (std::is_same_v<T, float>) {
        return 1.0f / (1.0f + expf(-x));
    } else {
        return static_cast<T>(1.0) / (static_cast<T>(1.0) + expf(-x));
    }
}

// Sigmoid Backward operator for MetaX backend
typedef struct SigmoidBackwardOp {
public:
    static constexpr size_t num_inputs = 2;

    template <typename T>
    __device__ __forceinline__ T operator()(const T &input, const T &grad_output) const {
        if constexpr (std::is_same_v<T, cuda_bfloat16>) {
            // High precision version: use double as intermediate calculation type
            float input_float = __bfloat162float(input);
            float grad_output_float = __bfloat162float(grad_output);
            
            double input_double = static_cast<double>(input_float);
            double grad_output_double = static_cast<double>(grad_output_float);
            
            double sigmoid_val = 1.0 / (1.0 + ::exp(-input_double));
            double result = grad_output_double * sigmoid_val * (1.0 - sigmoid_val);
            
            return __float2bfloat16(static_cast<float>(result));
        } else if constexpr (std::is_same_v<T, half>) {
            // For half precision, convert to float for calculation
            float input_f = __half2float(input);
            float grad_output_f = __half2float(grad_output);
            float sigmoid_val = 1.0f / (1.0f + expf(-input_f));
            float result = grad_output_f * sigmoid_val * (1.0f - sigmoid_val);
            return __float2half(result);
        } else {
            // For other types, use standard implementation with sigmoid_func
            T sigmoid_val = sigmoid_func(input);
            T one_minus_sigmoid = static_cast<T>(1.0) - sigmoid_val;
            return grad_output * sigmoid_val * one_minus_sigmoid;
        }
    }
} SigmoidBackwardOp;

Descriptor::~Descriptor() = default;

infiniStatus_t Descriptor::create(
    infiniopHandle_t handle_,
    Descriptor **desc_ptr,
    infiniopTensorDescriptor_t out_desc,
    std::vector<infiniopTensorDescriptor_t> input_desc_vec) {

    auto handle = reinterpret_cast<device::metax::Handle *>(handle_);
    auto dtype = out_desc->dtype();

    const auto &input_desc = input_desc_vec.at(0);
    const auto &grad_output_desc = input_desc_vec.at(1);
    const auto &y_shape = out_desc->shape();
    const auto &input_shape = input_desc->shape();
    const auto &grad_output_shape = grad_output_desc->shape();

    CHECK_DTYPE(dtype, INFINI_DTYPE_F16, INFINI_DTYPE_F32, INFINI_DTYPE_BF16);

    CHECK_SAME_SHAPE(y_shape, input_shape);
    CHECK_SAME_SHAPE(y_shape, grad_output_shape);

    // create METAX elementwise descriptor manually
    auto info_result = op::elementwise::ElementwiseInfo::create(out_desc, input_desc_vec);
    CHECK_RESULT(info_result);
    auto info = info_result.take();
    auto workspace_size = info.getMetaMemSize() + info.getInputSize() * sizeof(void *);

    auto device_impl_result = op::elementwise::metax::DeviceImpl::create(handle->internal());
    CHECK_RESULT(device_impl_result);

    *desc_ptr = new Descriptor(
        dtype,
        std::move(info),
        std::move(device_impl_result.take()),
        workspace_size,
        handle->device,
        handle->device_id);

    return INFINI_STATUS_SUCCESS;
}

infiniStatus_t Descriptor::calculate(
    void *workspace,
    size_t workspace_size,
    void *output,
    std::vector<const void *> inputs,
    void *stream) const {

    if (workspace_size < _workspace_size) {
        return INFINI_STATUS_INSUFFICIENT_WORKSPACE;
    }

    switch (_dtype) {
    case INFINI_DTYPE_F16:
        return _device_info->calculate<256, SigmoidBackwardOp, half>(_info, workspace, output, inputs, stream);
    case INFINI_DTYPE_F32:
        return _device_info->calculate<256, SigmoidBackwardOp, float>(_info, workspace, output, inputs, stream);
    case INFINI_DTYPE_BF16:
        return _device_info->calculate<256, SigmoidBackwardOp, cuda_bfloat16>(_info, workspace, output, inputs, stream);
    default:
        return INFINI_STATUS_BAD_TENSOR_DTYPE;
    }

    return INFINI_STATUS_SUCCESS;
}

} // namespace op::sigmoid_backward::metax